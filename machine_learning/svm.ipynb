{
 "metadata": {
  "name": "",
  "signature": "sha256:29922a98ef73cc27a11d11138e332ee23234ab275e9f5931a43d949440f912f3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#<span style=\"background-color:#66FF99\">Support Vector Machines<span/>\n",
      "<img src=\"../images/svm.png\" style=\"height:400px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Contents\n",
      "- [Background](#background)\n",
      "- [Titanic Dataset](#titanic-dataset)\n",
      "    - [Preliminaries](#preliminaries)\n",
      "    - [Load Data](#load-data)\n",
      "    - [Build Dataframe](#build-dataframe)\n",
      "    - [Fit Model](#fit-model)\n",
      "    - [Parameter Estimation](#parameter-estimation)\n",
      "    - [Model Evaluation](#model-evaluation)\n",
      "        - [Confusion Matrix](#confusion-matrix)\n",
      "        - [Precision-Recall](#precision-recall)\n",
      "        - [Prediction Probability](#prediction-probability)\n",
      "        - [ROC](#roc)\n",
      "    - [Comparison between Classifiers](#comparison-between-classifiers)\n",
      "    - [Feature Selection](#feature-selection)\n",
      "- [References](#references)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Background\n",
      "[back to top](#contents)\n",
      "\n",
      "###<span style=\"background-color:#FF99FF)\">Basics of Support Vector Machines</span>\n",
      "\n",
      "* One of the best 'out of box' classifers\n",
      "\n",
      "###<span style=\"background-color:#FF99FF)\">Maximal Margin Classifer</span>\n",
      "\n",
      "**What is a Hyperplane?**\n",
      "- Flat subspace of dimension p - 1 (in 3D, hyperplane is a flat 2D)\n",
      "- Hyperplane equation:\n",
      "\n",
      "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... \\beta_pX_p = 0$$\n",
      "\n",
      "**Classification using Separating Hyperplane**\n",
      "- A test observation is assigned a class depending on which side of the hyperplane it is located\n",
      "- If f(x) is positive, assign class 1, if negative, assign class -1\n",
      "- The magnitude of the f(x) shows how far it lies from the hyperplane\n",
      "- Separating hyperplane has the property:\n",
      "\n",
      "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... \\beta_pX_p > 0 \\rightarrow y_i = 1$$\n",
      "\n",
      "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... \\beta_pX_p < 0 \\rightarrow y_i = -1$$\n",
      "\n",
      "**Maximal Margin Classifer**\n",
      "- Which of the hyperplanes to use? Maximal margin hyperplane (aka. optimal separting hyperplane) where the margin is the largest\n",
      "- Margin is the smallest distance the observation is from the hyperplane\n",
      "- Support vectors: if these points are moved slighly, the maximal margin hyperplane would move\n",
      "\n",
      "**Construction of the Maximal Margin Classifier**\n",
      "- Optimization (using Math)!\n",
      "\n",
      "**Non-separable Case**\n",
      "- Maximal Margin Classifer is a natural way to perform classification, only if a separating hyperplane exists! Sometimes it does not\n",
      "- Develop a hyperplane that ALMOST separates the classes, Soft-Margin\n",
      "- Generalization of the maximal margin classifer to non-separable case is called ``Support Vector Classifer``\n",
      "\n",
      "###<span style=\"background-color:#FF99FF)\">Support Vector Classifers</span>\n",
      "\n",
      "**Overview**\n",
      "- A hyperplane which does not *perfectly* separate the 2 classes, just for *most* of training observations\n",
      "- Support Vector Classifers (Soft margin classifer) does this\n",
      "\n",
      "**Details of Support Vector Classifer**\n",
      "- Uses a nonnegative tuning parameter (C) which determines how much mistake we can tolerate (budget for the amount which can be violated\n",
      "- If C = 0, no tolerance, will be same as maximal margin hyperplane\n",
      "- If C > 0, no more than C observations can be on the wrong side\n",
      "- As C increases, the more tolerant, the bigger the margin\n",
      "- C is used as a tuning parameter and chosen via cross-validation  \n",
      "    \n",
      "###<span style=\"background-color:#FF99FF)\">Support Vector Machines</span>\n",
      "\n",
      "**Classification with Non-linear Decision Boundaries**\n",
      "- Support Vector Classifer useful for linear class boundaries. How about non-linear boundaries?\n",
      "- By using quadratic, cubic, higher-order polynomial\n",
      "- There are unlimited options and could end up with a huge number of features, then will be computationally expensive\n",
      "- SVM allows us to enlarge feature space which leads to efficient computations\n",
      "    \n",
      "* **Support Vector Machine**\n",
      "- Extension of the SVC using **Kernels** -  enlarge our feature space to accomodate a non-linear boundary\n",
      "- Kernel approach - efficient computational approach for enlarging\n",
      "- In representing the inner product and computing its coefficients, all we need are inner products (?)\n",
      "- Generalization of the inner product $K(x_i, x_i)$ where K is some function referred to as a kernel\n",
      "- Kernel is a function which quantifies the similarity of 2 observations!\n",
      "- Linear kernel quantifies the similarity of a pair of observations using Pearson (standard) correlation\n",
      "- Can also use polynomial kernel, radial kernel\n",
      "- How does radial kernel work? If given test observation is far from training observation, K will be very small. So training observations which are far will play no role is predicting the class. Radial kernel has local behavior, only nearby training observations wil have effect\n",
      "- What's the advantage of using Kernel as opposed to enlarging feature space? Computational less expensive!\n",
      "- Radial kernel takes various values of $\\gamma$ (positive constant) : as $\\gamma$ increase, fit becomes more linear\n",
      "\n",
      "###<span style=\"background-color:#FF99FF)\">SVMs with more than 2 classes</span>\n",
      "\n",
      "- One vs One Classification\n",
      "- One vs All Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Digits Dataset\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Preliminaries\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\")\n",
      "plt.rc(\"figure\", figsize=(10, 6))\n",
      "np.set_printoptions(precision=4)\n",
      "\n",
      "import warnings\n",
      "warnings.simplefilter('ignore', DeprecationWarning)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Load Data\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "\n",
      "# digits.head(5)\n",
      "digits.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Build Dataframe\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = digits\n",
      "\n",
      "X = df.data\n",
      "y = df.target\n",
      "n_samples, n_features = X.shape\n",
      "print(\"No. of samples = %d\" % n_samples)\n",
      "print(\"No. of features = %d\" % n_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA()\n",
      "X_pca = pca.fit(X)\n",
      "\n",
      "plt.title(\"Cumulated Explained Variance\")\n",
      "plt.ylabel(\"Percentage of explained variance\")\n",
      "plt.xlabel(\"PCA Components\")\n",
      "plt.plot(np.cumsum(X_pca.explained_variance_ratio_));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Fit Model\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
      "\n",
      "imputer = Imputer(strategy='mean', missing_values=-1)\n",
      "\n",
      "svc = SVC(kernel=\"rbf\",\n",
      "          C=10, \n",
      "          gamma=0.005)\n",
      "\n",
      "pipeline = Pipeline([\n",
      "    ('imp', imputer),\n",
      "    ('clf', svc),\n",
      "])\n",
      "\n",
      "cv = ShuffleSplit(n_samples, \n",
      "                  n_iter=50, \n",
      "                  train_size=500, \n",
      "                  test_size=500,\n",
      "                  random_state=0)\n",
      "\n",
      "scores = cross_val_score(pipeline, X, y, \n",
      "                         cv=cv, \n",
      "                         n_jobs=2,\n",
      "                         scoring='accuracy')\n",
      "\n",
      "print \"CV Score (min): %0.2f%%\" % (100 * scores.min())\n",
      "print \"CV Score (mean): %0.2f%%\" % (100 * scores.mean())\n",
      "print \"CV Score (max): %0.2f%%\" % (100 * scores.max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.kde import gaussian_kde\n",
      "_ = plt.hist(scores, range=(0, 1), bins=30, alpha=0.2)\n",
      "x = np.linspace(0, 1, 1000)\n",
      "smoothed = gaussian_kde(scores).evaluate(x)\n",
      "plt.plot(x, smoothed, label=\"Smoothed distribution\")\n",
      "top = np.max(smoothed)\n",
      "plt.vlines([np.mean(scores)], 0, top, color='r', label=\"Mean test score\")\n",
      "plt.vlines([np.median(scores)], 0, top, color='b', linestyles='dashed',\n",
      "           label=\"Median test score\")\n",
      "plt.legend(loc='best')\n",
      "_ = plt.title(\"Cross Validated test scores distribution\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Parameter Estimation\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc.get_params()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "# params = {'imp__strategy': ['mean', 'median'],\n",
      "#           'clf__C': np.logspace(-1, 2, 4),\n",
      "#           'clf__gamma': np.logspace(-4, 0, 5)\n",
      "#           }\n",
      "\n",
      "params = {'imp__strategy': ['mean', 'median'],\n",
      "          'clf__C': np.logspace(-1, 2, 4),\n",
      "          'clf__gamma': np.logspace(-4, 0, 5)}\n",
      "\n",
      "n_subsamples = 500\n",
      "X_small_train, y_small_train = X_train[:n_subsamples], y_train[:n_subsamples]\n",
      "\n",
      "gs= GridSearchCV(pipeline, params, cv=3, n_jobs=-1)\n",
      "gs.fit(X_small_train, y_small_train)\n",
      "\n",
      "print \"GS Best Score: %0.2f%%\" % (100 * gs.best_score_)\n",
      "print \"GS Best Params:\", gs.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sorted(gs.grid_scores_, \n",
      "#        key=lambda x: x.mean_validation_score, \n",
      "#        reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Model Evaluation\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Confusion Matrix\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "def plot_confusion(cm):\n",
      "    plt.imshow(cm, interpolation='nearest')\n",
      "    plt.title('Confusion matrix')\n",
      "    plt.set_cmap('Blues')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('True label')\n",
      "    plt.xlabel('Predicted label')\n",
      "    plt.tight_layout()\n",
      "\n",
      "y_pred = gs.predict(X_test)\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "cm_normalized = cm.astype(np.float64) / cm.sum(axis=1)\n",
      "\n",
      "plot_confusion(cm_normalized)\n",
      "# print cm_normalized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Precision-Recall\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "print(classification_report(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Prediction Probability\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred_proba = gs.predict_proba(X_test)\n",
      "y_pred_proba[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####ROC\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "\n",
      "def plot_roc_curve(y_test, y_pred_proba):\n",
      "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
      "    \n",
      "    roc_auc = auc(fpr, tpr)\n",
      "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
      "    plt.plot([0, 1], [0, 1], 'k--')\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.0])\n",
      "    plt.xlabel('False Positive Rate or (1 - Specificity)')\n",
      "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
      "    plt.title('Receiver Operating Characteristic')\n",
      "    plt.legend(loc=\"lower right\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_roc_curve(y_test, y_pred_proba)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Comparison between Classifiers\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = 3\n",
      "n_jobs = -1\n",
      "scoring = 'accuracy'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "logreg = LogisticRegression(C=1)\n",
      "pipeline = Pipeline([\n",
      "    ('imp', imputer),\n",
      "    ('clf', logreg)\n",
      "])\n",
      "\n",
      "scores = cross_val_score(pipeline, X, y, \n",
      "                         cv=cv, \n",
      "                         n_jobs=n_jobs,\n",
      "                         scoring=scoring)\n",
      "\n",
      "print \"Logistic Regression CV scores:\"\n",
      "print \"CV Score (min): %0.2f%%\" % (100 * scores.min())\n",
      "print \"CV Score (mean): %0.2f%%\" % (100 * scores.mean())\n",
      "print \"CV Score (max): %0.2f%%\" % (100 * scores.max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "rf = RandomForestClassifier(n_estimators=100)\n",
      "pipeline = Pipeline([\n",
      "    ('imp', imputer),\n",
      "    ('clf', rf)\n",
      "])\n",
      "\n",
      "scores = cross_val_score(pipeline, X, y, \n",
      "                         cv=cv, \n",
      "                         n_jobs=n_jobs,\n",
      "                         scoring=scoring)\n",
      "\n",
      "print \"Random Forest CV scores:\"\n",
      "print \"CV Score (min): %0.2f%%\" % (100 * scores.min())\n",
      "print \"CV Score (mean): %0.2f%%\" % (100 * scores.mean())\n",
      "print \"CV Score (max): %0.2f%%\" % (100 * scores.max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = GradientBoostingClassifier(n_estimators=100, \n",
      "                                        learning_rate=0.05,\n",
      "                                        max_features=.5,\n",
      "                                        max_depth= 5,\n",
      "                                        subsample=.8)\n",
      "\n",
      "pipeline = Pipeline([\n",
      "    ('imp', imputer),\n",
      "    ('clf', gb),\n",
      "])\n",
      "\n",
      "scores = cross_val_score(pipeline, X, y, \n",
      "                         cv=cv, \n",
      "                         n_jobs=n_jobs,\n",
      "                         scoring=scoring)\n",
      "\n",
      "print \"Gradient Boosted Trees CV scores:\"\n",
      "print \"CV Score (min): %0.2f%%\" % (100 * scores.min())\n",
      "print \"CV Score (mean): %0.2f%%\" % (100 * scores.mean())\n",
      "print \"CV Score (max): %0.2f%%\" % (100 * scores.max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Feature Selection\n",
      "[back to top](#contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg.fit(X, y)\n",
      "rf.fit(X, y)\n",
      "gb.fit(X, y)\n",
      "\n",
      "x = np.arange(len(df_features.columns))\n",
      "plt.bar(x, logreg.coef_.ravel(), color=\"#F08080\")\n",
      "_ = plt.xticks(x + 0.5, df_features.columns)\n",
      "plt.title('Logistic Regression: Coefficients')\n",
      "sns.despine()\n",
      "plt.figure()\n",
      "\n",
      "x = np.arange(len(df_features.columns))\n",
      "plt.bar(x, rf.feature_importances_)\n",
      "_ = plt.xticks(x + 0.5, df_features.columns)\n",
      "plt.title('Random Forest: Feature Importance')\n",
      "sns.despine()\n",
      "plt.figure()\n",
      "\n",
      "x = np.arange(len(df_features.columns))\n",
      "plt.bar(x, gb.feature_importances_)\n",
      "_ = plt.xticks(x + 0.5, df_features.columns, rotation=30)\n",
      "plt.title('Gradient Boosted Trees: Feature Importance')\n",
      "sns.despine()\n",
      "\n",
      "plt.tight_layout();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###References\n",
      "[back to top](#contents)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, HTML\n",
      "display(HTML(open('../style/custom.css').read()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}